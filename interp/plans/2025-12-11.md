# Interp Session Planner — Day 02
_Date: 2025-12-11

---

## 0. What I Took In Today (Upstream Context)

### 0.1 Theory / Math / Concepts
- PyTorch implementations of linear algebra, linear regression, logistic regression, weight decay, and softmax
- I'm a little unclear on logistic regression's exact implementation because I breezed through it, but I've learnt it before and so I still get the gist.

### 0.2 Implementation / Systems
- I built out a masked multi head attention mechansim.
- The build directory now actually has an attention mechanism I could import when I'm building a transformer.

---

## 1. Open Connection Brainstorm (Not a Decision Yet)
_This is a **divergent thinking** section. List multiple weak connections without judging them._

- “If today’s ideas were visible inside a model, they might show up as…”
  - Connecting to something I was thinking about yesterday, the notion of attention entropy could be cool to investigate.
  - It might be useful to try to find the induction heads in a toy model from scratch, without any LLM help, and then use an LLM to clean things up.
  - I don't really know how linear and logistic regression show up in a transformer, other than the basic ideas being carried over as inspiration.

- “If today’s implementation mattered mechanistically, it might be because…”
  - I wonder what the role of dropout is in mechanistic redundancy. How would I investigate this?
  - Could I intervene on the queries, keys, values somehow to control exactly what token information the model carries over that influences its next-token prediction?
  - If I have the sentence: "When it rains I carry an umbrella. When I carry an umbrella, I wear boots. When I don't carry an umbrella I wear sneakers. It rains, therefore I wear", could I, by intervening on the attention alone, change the model from predicting " boots", to predicting " sneakers"? Note that I'm not sold that this is necessarily the optimal prompt to test this.

_Write at least 3 candidate mechanisms, even if they feel dumb._

---

## 2. Today’s Interp Angle (Now We Converge)
_From the brainstorm above, pick **one thread** to pull on—not because it’s perfect, but because it’s testable today._

> **Chosen angle for today:**  
> Intervening on attention to switch a next-token prediction for a logical entailment task, from one implied enatilment to another.

---

## 3. Question Framed as Curiosity (Not Yet a Hypothesis)
_Not everything has to start as a clean falsifiable claim._

> Where does logical entailment come into play in attention?

---

## 4. What I Will Actually Touch (Concrete but Open)
- **Behavior / phenomenon of interest:**  
  Attention transformations relevant to logical entailment resolution.

- **Primary internal object(s):**  
  Attention heads/patterns/scores

This is allowed to be **vague at first** and sharpen during the session.

---

## 5. Provisional Hypothesis (Lightweight)
_Not a legal proof—just a directional belief._

> Certain heads/patterns connect the query token to the entailment resolving token, and switching those to point at a differnt token might switch the next-token prediction to a different entailment

Optional confidence: medium

---

## Gemini Feedback

You are testing induction heads and indirect object identification.

### Critical Flaws & Fixes
- The "Multi-Hop" Risk: Your prompt ("Rains -> Umbrella -> Boots") requires compositional attention. Small models often fail at this.
    - Fix: Use a medium-sized model.
- Mechanism Precision: Overwriting attention forces the model to *look* at a token. If the model relies on copying, this will work. If the logic is synthesized in the MLP layers, attention patching alone may fail.
    - Fix: Use nonsense words.

### Assumption Corrections
1. "I don't know how linear/logistic regression show up": They are the building blocks.
    - MLP Neurons: Linear Regression (wx + b)
    - Attention & Unembedding: Logistic Regression (Softmax probabilities).
2. "Logic happens in attention":
    - Attention *retrieves* operands.
    - MLP *computes* the function.
    - *Note*: You are intervening on the retrieval step, which is valid.

### The Protocol: Attention Pattern Patching
1. Cache: Run the clean prompt; sore the attention pattern (N x N matrix)
2. Identify: Find the head attending to "Boots" from the final position.
3. Intervene: Create a hook to overwrite that matrix during the forward pass.
    - Set score for index("Boots") $\approx 0$
    - Set score for index("Sneakers") $\approx 1$
4. Verify: Check if the output logit for "Sneakers" increases significantly.
