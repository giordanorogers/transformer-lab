# Interp Session Planner — Day 01
_Date: 2025-12-10

---

## 0. What I Took In Today (Upstream Context)
I read about linear algebra for DL.
I read about the tokenization and embedding process in transformers.
I also thought about prompt optimization, and hyperparameter sweeps.

### 0.1 Theory / Math / Concepts
- The idea that the language model dimensionality to bear in mind can be boiled down to `[Batch Size, Sequence Length, Model Dimension]` stood out as something in my linear algebra notes as well as in the tokenization and embedding chapter.
- The idea of decomposing matrices via eigendecomposition and singular value decomposition stood out. It makes me wonder how exactly these apply to interpretability techniques. If I collect a matrix of activations in my model, or token embeddings, what can I understand about them through decomposition?
- I pretty much breezed through the mathematical explanation of PCA. That's something I'll want to learn at some point but felt skippable for now, or at least, breeze-through-able.
- I wonder what the Moore-Penrose Pseudoinverse's utility is in interpretability.

### 0.2 Implementation / Systems
- I built a super simple tokenizer.
- I built a super simple absolute positional embdding matrix.
- It felt good to write notes after reading the linear algebra chapter using Gemini's help. Connecting thins to transformers felt good.
- The codebase still feels empty.

---

## 1. Open Connection Brainstorm (Not a Decision Yet)
_This is a **divergent thinking** section. List multiple weak connections without judging them._

- “If today’s ideas were visible inside a model, they might show up as…”
  - I imagine there could be an interesting affect on attention or embedding or activation geometry which occurs when you vary the type of embedding strategy you use, whether it be absolute, relative, rotary, etc. It might be pretty hard to test this though without a model that I could plug and play these different embeddings with.
  - I also, as I wrote above, wonder what the utility of different decompositions are for interpretability.

- “If today’s implementation mattered mechanistically, it might be because…”
  - The different choices for positional encoding certainly matter mechanistically.
  - The different decomposition strategies certainly have different use cases when it comes to interp.

---

## 2. Today’s Interp Angle (Now We Converge)
_From the brainstorm above, pick **one thread** to pull on—not because it’s perfect, but because it’s testable today._

> **Chosen angle for today:**  
> A dive into how to use different decomposition strategies along with PCA for analyzing and visualizing layer-by-layer embeddings/activations.

---

## 3. Question Framed as Curiosity (Not Yet a Hypothesis)
_Not everything has to start as a clean falsifiable claim._

> What are different decomposition strategies good for?
> How might they be useful when it comes to analyzing activations?

---

## 4. What I Will Actually Touch (Concrete but Open)
- **Behavior / phenomenon of interest:**  
  How to use decomposition techniques to analyze layer-by-layer activations and/or embeddings

- **Primary internal object(s):**  
  Embeddings/activations

This is allowed to be **vague at first** and sharpen during the session.

---

## 5. Provisional Hypothesis (Lightweight)
_Not a legal proof—just a directional belief._

> No hypotheses, simply exploratory

---

## 6. Intervention Menu (Pick What Feels Most Informative Today)
_Check any that feel relevant—you’ll choose one after this._

- [ ] Ablate / zero something
- [ ] Patch activations between prompts/examples
- [ ] Add noise or rescale
- [ ] Read out with logit lens or unembedding
- [√] Compare geometries (PCA, cosine, clustering)
- [√] Compare across layers
- [√] Compare across inputs
- [ ] Other: {{ }}

---

## 7. Measurement Menu (Also Open)
_I don’t have to choose yet, but I must end up with at least one number and one picture._

- Possible **numbers**:
  - {{ Δ logit }}
  - {{ Δ loss }}
  - {{ accuracy change }}
  - {{ rank shift }}
  - {{ cosine similarity }}
  - {{ variance explained }}

- Possible **visuals**:
  - {{ PCA projection }}
  - {{ token trajectory }}

---

## 8. Session Success = One Clear Answer to One Clear Question
> “At the end of this session, I want to be able to say:  
> I understand when to use different decomposition strategies for activation/embedding analysis.

---

## 9. Scope Guardrails (Still Non-Negotiable)
- ✅ One main question
- ✅ One primary component family
- ✅ One intervention type actually executed
- ✅ One metric + one figure produced
- ✅ Stop at 1 hour even if things get interesting

---

### 1. Eigendecomposition & SVD: Finding "Concepts" and "Circuits"

In interpretability, we assume that high-dimensional vectors (length 768 in GPT-2 Small) are actually combinations of simpler "features" or "concepts".

#### SVD on Weight Matrices (The "Circuit" View):

- Every Attention Head has an Output Matrix ($W_O$) and a Value Matrix ($W_V$). We often multiply them to see the "OV Circuit" ($W_{OV} = W_V W_O$).
- If you perform SVD on $W_{OV}$, you often find it is "low rank". This means the head is only writing to a tiny subspace of the residual stream.
- Pragmatic Interp: If the top singular value is huge and the rest are near zero, that head is likely just copying specific information (like "if previous token is 'John', write 'He').

#### PCA on Activations (The "Geometry" View):

- If you collect the activations of 100 sentenes, you have a massive cloud of data.
- PCA (which is mathematically built on Eigendecomposition/SVD) rotates that cloud to show you the "shape" of the model's thinking.
- Pragmatic Interp: You can see if the model groups sentences by topic (e.g., all "French" sentences cluster together, all "Code" sentences cluster together) in early layers versus late layers.

### 2. Moore-Penrose Pseudoinverse: The "Probe" Builder

In interp, this is the engine behind Linear Probes.

- The Problem: You have a stack of activations $A$ (inputs) and you want to know if the model knows a specific fact (labels $Y$, e.g., "is this sentiment positive?")
- The Math: You want to find a direction vector $w$ such that $Aw \approx Y$.
- The Solution: Since $A$ is rarely a perfect square matrix with an inverse, you can't do $w=A^{-1}Y$. Instead, you use the pseudoinverse: $w=A^{+}Y$.
- Pragmatic Interp: If you want to find the "Truth Direction" in a model, you mathematically solve for the vector that best separates "True" statements from "False" statements using the pseudoinverse.

## The Experiment Plan

Goal: Visualizing the "Thought Trajectory".

Hypothesis: Similar inputs should have similar trajectories through the layers, while distinct inputs should diverse.