# Day 01 – Daily Research Log
_Date: 2025-12-10

## 0. Metadata (5 min)
- **Start time:** ~4:00
- **End time:** 9:26
- **Total focused work today (hrs):** 5.5
- **Main focus today (1 sentence):**  
  Compared different probing methods to see which one could extract a concept and affect concept erasure better.

---

## 1. Raw Dump – What Actually Happened (30 min)
I started off by reading the Linear Algebra chapter in Deep Learning. It reminded me of the core things I need to know. It got my mind thinking about decomposition: what it means to decompose the different matrices one might deal with in interpretability experiments.

Then I moved on to reading Build a LLM from Scratch. I breezed through chapter one and focused more on chapter 2, which had to do with tokenization and embeddings. It got me thinking about the effects of different positional encoding strategies.

Then I worked on a simple interp experiment looking at how the Moore-Penrose Pseudoinverse could be used for linear probing. Turns out, probing with the pseudoinverse is just linear regression. When I saw that kind of work, it got me thinking about what all the major probing strategies are and how the pseudoinverse method compares. So I tried difference of means, logistic regression, and support vector machines. At least on the setup I had, difference of means was the only one that had no effect. Granted, my dataset was tiny.

---

## 2. Structured Experiment Record (30 min)
_(Compress the mess above into one clean “experiment block.” If you ran multiple, pick the most important.)_

### 2.1 Setup
- **Target behavior:**  
  Change next-token prediction behavior to remove past tensing.
- **Internal component:**  
  Residual stream at layer 6.
- **Hypothesis (falsifiable):**  
  Collecting a probe for different categoris of prompts and using that to construct a projection matrix would allow me to multiply the hidden state at a given layer and change the model's behavior to eliminate its ability to perform a specific behavior.
- **Intervention type:**  
  - [ ] Head ablation  
  - [ ] MLP ablation  
  - [] Residual patching  
  - [ ] Logit lens probe  
  - [ ] Activation swap  
  - [√] Other: Activation projection

### 2.2 Procedure
_1–3 bullet points, concrete steps only._
- Collect activations on two different categories of prompts.
- Construct a projection matrix using a direction found with a linear probing technique.
- Project the hidden state at a given layer for a test prompt onto that projection matrix and see how it affects downstream behavior.

### 2.3 Measurements
- **Metrics recorded (with units):**  
  - Behvaior, not metric.

- **Plots/figures generated (with filenames):**  
  - No plots.

### 2.4 Observed Effect (1–3 sentences)
_Write like a result in a paper, not a diary entry._

> Projecting onto matrices gathered with linear regression, logistic regression, and SVM had an effect half of the time. Difference of means didn't work.

---

## 3. Claim / Rule Extraction (30 min)
_(This is the “turning evidence into knowledge” section.)_

### 3.1 Today’s Main Claim (pick exactly one)
Fill in ONE of these, delete the others:

- **Broken/updated hypothesis:**  
  > I previously believed that Difference of Means was always the best probing method because of the Geometry of Truth paper. Now I know that other methods work better sometimes.

### 3.2 How confident am I in this claim? (0–1)
- **Confidence:** 1.0
- **Why this confidence level?**  
  I saw the behavior change, and lack therof, myself.

---

## 4. Integration with Bigger Picture
### 4.1 How does today’s result connect to previous days?
- It's day one baby.

### 4.2 What does this suggest about transformer computation?
- Trying to gather a direction that represents a single concept and intervene with it can be tricky.

---

## 5. Plan for Tomorrow (5–10 min)
Keep this brutally concrete.

### 5.1 Next Experiment
- **Hypothesis to test:**  
  Unsure yet. Probably something to do with attention, since that's what I'll be learning about in LLM. Might want to think about attention through a probability theory or information theory lens, since that's what I'll be learning about in DL.
- **Component(s) to intervene on:**  
  There's this notion of attention entropy that it might be fun to learn about and test out. I'll probably want to do some circuitsvis stuff and check out attention patterns and see what probabilities its assigning to differnt tokens on a certain task and see how this relates to the notion of attention entropy.
- **Intervention type:**  
  Might intervene on $W_{OV}$, since that was something I had considered doing today but didn't.
- **Success criterion:**  
  Hopefully something I do can affect the attention pattern.

### 5.2 Risks / Confusions
- Nope.

---

## 6. Quick Self-Check (2 min)
_(Checkboxes to keep you honest.)_

- [ ] Did I write down at least one **falsifiable hypothesis**?
- [ ] Did I log at least one **metric** (number) and one **figure** (path)?
- [√] Did I extract **one main claim** for today?
- [ ] Did I define **one concrete thing** to test tomorrow?

---

## 7. Optional: Mental State Snapshot (totally optional)
- **Energy level (0–10):** 6
- **Frustration level (0–10):** 2
- **One sentence about how I feel about the work right now:**  
  I feel good, although, I wonder how much my current process will stick. I worry I might be using LLM-assistance too much. But it's time efficient and I would get stuck in headaches more if I didn't. So I'll continue the current workflow I have and perhaps consider readjusting in a few days.