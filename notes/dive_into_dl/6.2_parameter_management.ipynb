{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00753e54",
   "metadata": {},
   "source": [
    "In this section, we cover:\n",
    "- Accessing parameters for debugging, diagnostics, and visualizations\n",
    "- Sharing parameters across different model componetns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf81cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f4c242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/d2l/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.LazyLinear(8),\n",
    "    nn.ReLU(),\n",
    "    nn.LazyLinear(1)\n",
    ")\n",
    "\n",
    "X = torch.rand(size=(2,4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4949c881",
   "metadata": {},
   "source": [
    "When w model is defined via the Sequential class, we can first access any layer by indexing into the model as though it were a list. Each layer's parameters are conveniently located in its attribute.\n",
    "\n",
    "We can inspect the parameters of the second fully connected layer as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265c684c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.1851, -0.0598,  0.1091, -0.0429, -0.0689,  0.2480, -0.1503, -0.1469]])),\n",
       "             ('bias', tensor([0.0506]))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ce34c",
   "metadata": {},
   "source": [
    "We can see that this fully connected lauer contains two parameters, corresponding to that layer's weights and biases, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290aee53",
   "metadata": {},
   "source": [
    "Note that each parameter is represented as an instance of the parameter class. To do anything useful with parameters, we first need to access the underlying numerical values. There are several ways to do this. Some are simpler while others are more general. The following code extracts the bias from the second neural network layer, which returns a parameter class instance, and further accesses that parameter's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa5d7e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.nn.parameter.Parameter, tensor([0.0506]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net[2].bias), net[2].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14655287",
   "metadata": {},
   "source": [
    "Parameters are complex objects, containing values, gradients, and additional information. That is why we need to request the value explicitly.\n",
    "\n",
    "In addition to the value, each parameter also allows us to access the gradient. Because we have not invoked backpropagation for this network yet, it is in its initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70bc9199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4129c264",
   "metadata": {},
   "source": [
    "When we need to perform operations on all parameters, accessing them one-by-one can grow tedious. The situation can grow especially unweildy when we work with more complex, e.g., nested, modules, since we would need to recurse through the entire tree to extract each sub-module's parameters. Below we demonstrate accessing the parameters of all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b82fc065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight', torch.Size([8, 4])),\n",
       " ('0.bias', torch.Size([8])),\n",
       " ('2.weight', torch.Size([1, 8])),\n",
       " ('2.bias', torch.Size([1]))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, param.shape) for name, param in net.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9f90e",
   "metadata": {},
   "source": [
    "Often, we want to share parameters across multiple layers. Let's see how to do this elegantly. In the following we allocate a fully connected layer and then use its parameters specifically to set those of another layer. Here we need to run the forward propagation net(X) before accessing the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c0daea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/d2l/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# We need to give the shared layer a name so that we can refer to its parameters\n",
    "shared = nn.LazyLinear(8)\n",
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "net(X)\n",
    "# Chekc whether the parameters are the same\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] == 100\n",
    "# Make sure theat they are actually the same object rather than just having the same value\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4115be7",
   "metadata": {},
   "source": [
    "This example shows that the parameters of the second and third layer are tied. They are not just equal, they are represented by the exact same tensor. Thus, if we change one of the parameters, the other one changes too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b85893",
   "metadata": {},
   "source": [
    "You might wonder, when parameters are tied, what happens to the gradients? Since the model parameters contain gradients, the gradients of the second hidden layer and the third hidden layer are added together during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2301d6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
