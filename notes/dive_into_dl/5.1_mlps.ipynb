{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b236d5",
   "metadata": {},
   "source": [
    "## Hidden Layers\n",
    "\n",
    "We described affine transformations as linear transformations with added bias. To begin, recall the model architecture corresponding to our softmax regression example. This model maps input directly to outputs via a single affine transformation, followed by a softmax operation. If our labels were truly related to the input data by a simple affine transformation, then this approach would be sufficient. However, linearity (in affine transformations) is a *strong* assumption.\n",
    "\n",
    "### Limitations of Linear Models\n",
    "\n",
    "For example, linearity implies the *weaker* assumption of *monotonicity*, i.e., that any increase in our feature must either always cause an increase in our model's output (if the corresponding weight is positive), or always cause a decrease in our model's output (if the corresponding weight is negative). Sometimes that makes sense. For example, if we were trying to predict whether an individual will repay a loan, we might reasonably assume that all other things being equal, an applicant with a higher income would always be more likely to repay than one with a lower income.\n",
    "\n",
    "### Universal Approximators\n",
    "\n",
    "Even with a single-hidden-layer network, given enough nodes (possibly absurdly many), and the right set of weights, we can model any function. Actually learning that function is the hard part, though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eea5bb",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Activation functions decide whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. They are differentiable operators for transforming input signals to outputs, while most of them add nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f24c6e",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "Given an element $x$, the function is defined as the maximum of that element and 0.\n",
    "\n",
    "Informally, the ReLU function retains only positive elements and discards all negative elements by setting the corresponding activations to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30219963",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\n",
    "y = torch.relu(x)\n",
    "d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ca79d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
