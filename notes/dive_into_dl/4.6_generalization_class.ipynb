{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d67b958",
   "metadata": {},
   "source": [
    "It turns out that we often can guarantee generalization a priori: for many models, and for any desired upper bound on the generalization gap $\\epsilon$, we can often determine some required number of samples $n$ such that if our training set contains at least $n$ samples, our empirical error will lie within $\\epsilon$ of the true error, for any data generating distribution. Unfortunately, it also turns out that while these sorts of guarantees provide a profound set of intellectual building blocks, they are of limited practical utility to the deep learning practitioner. In short, these guarantees suggest that ensuring generalization of deep neural networks a priori requires an absurd number of examples (perhaps trillions or more), even when we find that, on the tasks we care about, deep neural networks typically generalize remarkably well with far fewer examples (thousands). Thus deep learning practitioners often forgo a priori guarantees altogether, instead employing methods that have generalized well on similar problems in the past, and certifying generalization post hoc through empricial evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97852036",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
