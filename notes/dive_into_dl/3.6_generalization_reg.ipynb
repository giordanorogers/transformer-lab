{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b630420c",
   "metadata": {},
   "source": [
    "The phenomenon of fitting closer to our training data than to the underlying distribution is called overfitting, and techniques for combating overfitting are often called regularization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d321b0f8",
   "metadata": {},
   "source": [
    "In the standard supervised learning setting, we assume that the training data and the test data are drawn independently from identical distributions. This is commonly called the IID assumption.\n",
    "\n",
    "We need to differentiate between the training error $R_{emp}$, which is a statistic calculated on the training dataset, and the generalization error $R$, which is an expectation taken with respect to the underlying distribution. You can think of the generalization error as what you would see if you applied your model to an infinite stream of additional data examples drawn from the same underlying data distribution.\n",
    "\n",
    "Problematically, we can never calculate the generalization error $R$ exactly. Nobody ever tells us the precise form of the density function $p(x, y)$. Moreover, we cannot sample an infinite stream of data points. Thus, in practice, we must estimate the generalization error by applying our model to an independent test set constituted of a random selection of examples $\\mathbf{X}'$ and labels $\\mathbf{y}'$ that were withheld from our training set. This consists of applying the same formula that was used for calculating the empirical training error but to a test set $\\mathbf{X}', \\mathbf{y}'$.\n",
    "\n",
    "Crucuially, when we evaluate our classifier on the test set, we are working with a fixed classifier (it does not depend on the sample of the test set), and thus estimating its error is simply the problem of mean estimation. However the same cannot be said for the training set. Note that the model we wind up with depends explicitly on the selection of the training set and thus the training error will in general be a biased estimate of the true error on the underlying population. The central question of generalization is then when should we expect our training error to be close to the population error (and thus the generalization error)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b17d7",
   "metadata": {},
   "source": [
    "In classical theory, when we have simple models and abundant data, the training and generalization errors tend to be close. However, when we work with more complex models and/or fewer examples, we expect the training error to go down but the generalization gap to grow. This should not be surprising. Imagine a model class so expressive that for any dataset of n examples, we can find a set of parameters that can perfectly fit arbitrary labels, even if randomly assigned. In this case, even if we fit our training data perfectly, how can we conclude anything about the generalization error? For all we know, our generalization error might be no better than random guessing.\n",
    "\n",
    "In general, absent any restriction on our model class, we cannot conclude, based on fitting the training data alone, that our model has discovered any generalizable pattern. On the other hand, if our model class was not capable of fitting arbitrary labels, then it must have discovered a pattern. Learning-theoretic ideas about model complexity derived some inspiration from Karl Popper, who formalized the criterion of falsifiability. According to Popper, a theory that can explain any and all observations is not a scientific theory at all! After all, what has it told us about the world if it has not ruled out any possibility? In short, what we want is a hypothesis that could not explain any observations we might conceivably make and yet nevertheless happens to be compatible with those observations that we in fact make.\n",
    "\n",
    "Now what precisely constitutes an appropriate notion of model complexity is a complex matter. Often, models with more parameters are able to fit a greater number of arbitrarily assigned labels. However, this is not necessarily true. For instance, kernel methods operate in spaces with infinite numbers of parameters, yet their complexity is controlled by other means. One notion of complexity that often proves useful is the range of values that the parameters can take. Here, a model whose parameters are permitted to take arbitrary values would be more complex.\n",
    "\n",
    "When a model is capable of fitting arbitrary labels, low training error does not necessarily imply low generalization error. However, it does not necessarily imply high generalization error either! All we can say with confidence is that low training error alone is not enough to certify low generalizatino error. Error on the holdout data, i.e., validation set, is called the validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa240e",
   "metadata": {},
   "source": [
    "Fixing out model, the fewer samples we have in the training dataset, the more likely (and more severely) we are to encounter overfitting. As we increase the amount of training data, the generalization error typically decreases. Moreover, in general, more data never hurts. For a fixed task and data distribution, model complexity should not increase more rapidly than the amount of data. Given more data, we might attempt to fit a more complex model. Absent sufficient data, simpler models may be more difficult to beat. For many tasks, deep learning only outperforms linear models when many thousands of training examples are available. In part, the current success of deep learning owes considerably to the abundance of massive datasets arising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45611c",
   "metadata": {},
   "source": [
    "Typically, we select our final model only after evaluating multiple models that differ in various ways (different architectures, training objectives, selected features, data preprocessing, learning rates, etc.). Choosing among many models is aptly called model selection.\n",
    "\n",
    "In principle, we should not touch our test set until after we have chosen all our hyperparameters. Were we to use the test data in the model selection process, there is a risk that we might overfit the test data. Then we would be in serious trouble. If we overfit our training data, there is always the evaluation on test data to keep us honest. But if we overfit the test data, how would we ever know?\n",
    "\n",
    "Thus, we should never rely on the test data for model selection. And yet, we cannot rely solely on the training data for model selection either because we cannot estimate the generalization error on the very data that we use to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bfce1",
   "metadata": {},
   "source": [
    "When training data is scarce, we might not even be able to afford to hold out enough data to constitute a proper validation set. One popular solution to this problem is to employ K-fold cross-validation. Here, the original training data is split into K non-overlapping subsets. Then model training and validation are executed K times, each time training on K-1 subsets and validating on a different subset (the one not used for training in that round). Finally, the training and validation errors are estimated by averaging over the results from the K experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ba5ae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
