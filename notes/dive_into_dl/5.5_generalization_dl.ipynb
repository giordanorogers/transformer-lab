{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6dd0688",
   "metadata": {},
   "source": [
    "## Revisiting Overfitting and Regularization\n",
    "\n",
    "Accoring to the \"no free lunch\" theorem, any learning algorithm generalizes better on data with certain distributions, and worse with other distributions. Thus, given a finite training set, a model relies on certain assumptions: to achieve human-level performance it may be useful to identify *inductive biases* that reflect how humans think about the world. Such inductive biases show preferences for solutions with certain properties. For example, a deep MLP has an inductive bias towards building up a complicated function by the composition of simpler functions.\n",
    "\n",
    "With ML models encoding inductive biases, our approach to training them typically consists of two phases: (i) fit the training data; and (ii) estimate the generalization error (the true error on the underlying population) by evaluating the model on holdout data. The difference between our fit on the training data and our fit on the test data is called the generalization gap and when this is large, we say that our models overfit to the training data. In extreme extreme cases of overfitting, we might exactly fit the training data, even when the test error remains significant/ And in the classicial view, the interpretation is that our models are too complex, requiring that we either shrink the number of features, the number of nonzero parameters learned, or the size of the parameters as quantified.\n",
    "\n",
    "However deep learning complicates this picture in counterintuitive ways. First, for classification problems, our models are typically expressive enough to perfectly fit every training example, even in datasets consisting of millions. IN the classical picture, we might think that this setting lies on the far right extreme of the model complexity axis, and that any improvements in generalization error must come by way of regularization, either by reducing the complexity of the model class, or by applying a penalty, severely constraining the set of values that our parameters might take. But this is where things start to get weird.\n",
    "\n",
    "Strangely, for many deep learning tasks (e.g., image recognition and text classification) we are typically choosing among model architectures, all of which can achieve arbitrarily low training loss (and zero training error). Because all models under consideration achieve zero training error, the only avenue for further gains is to reduce overfitting. Even strangeer, it is often the case that despite fitting the training data perfectly, we can actually reduce the generalization error further by making the model even more expressive, e.g., adding layers, nodes, or training for a larger number of epochs. Stranger yet, the pattern relating the generalization gap to the complexity of the model (as captured, for example, in the depth or width of the networks) can be non-monotonic, with greater complexity hurting at first but subsequently helping in a so-called \"double descent\" pattern. Thus the deep learning practitioner possesses a bag of tricks, some of which seemingly restrict the model in some fashion and others that seemingly make it even more expressive, and all of which, in some sense, are applied to mitigate overfitting.\n",
    "\n",
    "Complicating things even further, while the guarantees provided by classical learning theory can be conservative even for classical models, they appear powerless to explain why it is that deep neural networks generalize in the first place. Because deep neural networks are capable of fitting arbitrary labels even for large datasets, and despite the use of familiar methods such as l2 regularization, traditional complexity-based generalization bounds, e.g., those based on the VC dimension or Rademacher complexity of a hypothesis class cannot explain why neural networks generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4faa8f",
   "metadata": {},
   "source": [
    "## Inspiration from Nonparametrics\n",
    "\n",
    "Approaching deep learning for the first time, it is tempting to think of them as parametric models. After all, the models do have millions of parameters. When we update the models, we update their parameters. When we save the models, we write their parameters to disk. However, mathematics and computer science are riddled with counterintuitive changes of perspective, and surprising isomorphisms between seemingly different problems. While NNs clearly have parameters, in some ways it can be more fruitful to think of them as behaving like nonparameteric models. So what precisely makes a model nonparametric? While the name covers a diverse set of approaches, one common theme is that nonparametric methods tend to have a level of complexity that grows as the amount of available data grows.\n",
    "\n",
    "Perhaps the simplest example of a nonparametric model is the KNN algorithm. Here, at training time, the learning simply memorizes the dataset. Then, at prediction time, when confronted with a new point x, the learner looks up the k nearest neighbors (the k points x'_i that minimizie some distance d(x, x'_i)). When k=1, this algorithm is called 1-nearest neighbors, and the algorithm will always achieve a training error of zero. That however, does not mean that the algorithm will not generalize. In fact, it turns out that under some mild conditions, the 1-nearest neighbor algorithm is consistent (eventually converging to the optimal predictor).\n",
    "\n",
    "Note that 1-nearest neighbor requires that we specify dome distance function d, or equivalently, that we specify some vector-valued basis function $\\phi(x)$ for featurizing our data. For any choice of the distance metric, we will achieve zero training error and eventually reach an optimal predictor, but different distance metrics d encode diffeerent inductive biases and with a finite amount of available data will yield different predictors. Different choices of the distance metric d represent different assumptions about the underlying patterns and the performance of the different predictors will depend on how compatible the assumptions are with the observed data.\n",
    "\n",
    "In a sense, because neural networks are over-parameterized, possessing many more parameters than are needed to fit the training data, they tend to interpolate the training data (fitting it perfectly) and thus behave, in some ways, more like nonparameteric models. More recent theoretical research has established deep connection between large neural networks and nonparametric methods, notably kernel methods. In particular, Jacot et al demonstrated that in the limit, as multiplayer perceptrons with randomly initialized weights grow infinitely wide, they become equivalent to (nonparametric) kernel methods for a specifdic choice of the kernel function (essentially, a distance function), which they call the nerual tangent kernel. While current neural tangent kernel methods may not fully explain the behavior of modern deep networks, their success as an analytical tool underscores the usefulness of nonparametric modeling for understanding the behavior of over-parameterized deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ccbb93",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "While deep neural networks are capable of fitting arbitrary labels, even when labels are assigned incorrectly or randomly, this capability only emerges over many iterations of training. A new line of work has revealed that in the setting of label noise, neural networks tend to fit cleanly labeled data first and only subsequently to interpolate the mislabeled data. Moreover, it has been established that this phenomenon translates directly into a guarantee on generalization: whenever a model has fitted the cleanly labeled data but not randomly labeled examples included in the training set, it has in fact generalized.\n",
    "\n",
    "Together these findings help to motivate early stopping, a classic technique for regularizing deep neural networks. Here, rather than directly constraining the values of the weights, one constrains the number of epochs of training. The most common way to determine the stopping criterion is to monito validation error throughout training (typically by checking once after each epoch) and to cut off training when the validation error has not decreased by more than some small amount $\\epsilon$ for some number of epochs. This is sometimes called a patience criterion. As well as the potential to lead to better generalizaiton in the setting of noisy labels, another benefit of early stopping is time saved. Once the patience criterion is met, one can terminate training.\n",
    "\n",
    "Notably, when there is no label noise and datasets are realizable (the classes are truly separable, e.g., distinguishing cats from dogs), early stopping tends not to lead to significant improvements in generalization. On the other hand, when there is label noise, or intrinsic variability in the label (e.g., predicting mortality among patients), early stopping is crucial. Training models until they interpolate noisy data is typically a bad idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b7a00",
   "metadata": {},
   "source": [
    "## Classical Regularization Methods for Deep Networks\n",
    "\n",
    "In section 3, we described several classical regularization techniques for constraining the complexity of our models. IN particular, Seciton 3.7 introduced a method called weight decay, which consists of adding a regularization term to the loss function in order to penalize large values of the weights. Depending on which weight norm is penalized this technique is known either as ridge regularization (for l2 penalty) or lasso regularization (for an l1 penalty). In the classical analysis of these regularizers, they are considered as sufficiently restrictive on the value sthat the weights can take to prevent the model from fitting arbitrary labels.\n",
    "\n",
    "In deep learning implementations, weight decay remains a popular tool. However, researchers have noted that typical strengths of l2 regularization are insufficient to prevent the networks from interpolating the data and thus the benefits if interpreted as regularization might only make sense in combination with the early stopping criterion. Absent early stopping, it is possible that just like the number of laeyrs or number of nodes (in deep learning) or the distance metric (in 1-nearest neighbor), these mthods may lead to better generalization not because they meaningfully constrain the power of the neural network but rather because they somehow encode inductive biases that are better compatible with the patterns found in datasets of interest. Thus, classical regularizers remain popular in deep learning implementations, even if the theoretical raionale for their efficacy may be radically different.\n",
    "\n",
    "Notably, DL researchers have also built on techniques first popularized in classical regularization contexts, such as adding noise to model inputs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
