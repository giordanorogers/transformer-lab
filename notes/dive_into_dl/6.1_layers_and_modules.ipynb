{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3250fe7a",
   "metadata": {},
   "source": [
    "A module could describe a single layer, a component consisting of multiple layers, or the entire model itself! One benefit of working with the module abstraction is that they can be combined into larger artifacts, often recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d257248",
   "metadata": {},
   "source": [
    "From a programming standpoint, a module is represented by a class. Any subclass of it must define a forward propagation method that transforms its input into output and must store any necessary parameters. Note that some modules do not require any parameters at all. Finally a module must possess a backpropagation method, for the purposes of calculating gradients. Fortunately, due to some behind-the-scenes magic supplied by the auto-differentiation when defining our own module, we only need to worry about parameters and the forward propagation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eaf594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eadf3946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/d2l/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generates a network with one fully connected hidden layer with 256 units and ReLU activation\n",
    "# followed by a fully connected output layer with ten units (no activation function)\n",
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bf27bd",
   "metadata": {},
   "source": [
    "In this example, we constructed our model by instantiating an nn.Sequential, with layers in the order that they should be executed passed as arguments. In short, nn.Sequential defines a specific kind of Module, the class that presents a module in PyTorch. It maintains an ordered list of consistent Modules. Note that each of the two fully connected layers is an instance of the Linear class which itself is a subclass of Module. The forward propagation method is also remarkably simple: it chains each module in the list together, passing the output of each as input to the next. Note that until now, we have been invoking our models via the construction net(X) to obtain their outputs. This is acutally just shorthand for net.__call__(X)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe027b92",
   "metadata": {},
   "source": [
    "The basic functionality that each module must provide:\n",
    "\n",
    "1. Ingest input data as arguments to its forward propagation method.\n",
    "2. Generate an output by having the forward propagation method return a value. Note that the output may have a different shape from the input. For example, the first fully connected layer in out model above ingests an input of arbitrary dimension but returns an output of dimension 256.\n",
    "3. Calculate the gradient of its output with respect to its input, which can be accessed via it backpropagation method. Typically this happens automatically.\n",
    "4. Store and provide access to those parameters necessary for executing the forward propagation computation.\n",
    "5. Initialize model parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44842156",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class nn.Module to perform\n",
    "        # the necessary initialization\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.out = nn.LazyLinear(10)\n",
    "        \n",
    "    # Define the forward propagation of the model, that is, how to return the\n",
    "    # required model output based on the input X\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f728c4",
   "metadata": {},
   "source": [
    "Let's first focus on the forward propagation method. Note that it takes X as input, calculates the hidden representation with the activation function applied, and outputs its logits. In this MLP implementation, both layers are instance variables. To see why this is reasonable, imaging instantiating two MLPs, net1 and net2, and training them on different data. Naturally, we would expect them to represent two different learned models.\n",
    "\n",
    "We instantiate the MLP's layers in the constructor and subsequently invoke these layers on each call to the forward propagation method. NOte a few key details. First, out customized __init__ method invokes the parent class's __init__ method via super().__init__() sparing us the pain of restating boilerplate code applicable to most modules. We then instantiate our two fully connected layers, assigning them to self.hidden and self.out. Note that unless we implement a new layer, we need not worry about the backpropagation method or parameter initializaiton. The system will generate these methods automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe0c3cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/d2l/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b7514",
   "metadata": {},
   "source": [
    "To build out own simplified MySequential, we just need to define two key methods:\n",
    "\n",
    "1. A method for appending modules one by one to a list.\n",
    "2. A forward propagation method for passing an input through the chain of modules, in the same order as they were appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba781bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        for module in self.children():\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f91e976",
   "metadata": {},
   "source": [
    "In the __init__ method, we add every module by calling the add_mdoules method. These modules can be accessed by the children method at a later data. In this way the system knows the added modules, and it will properly initialize each module's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc91289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/d2l/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94c62b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20))\n",
    "        self.linear = nn.LazyLinear(20)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa36db03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
